{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc6c810",
   "metadata": {},
   "source": [
    "### Преамбула к ДЗ-1\n",
    "\n",
    "В рамках даной работы вам нужно будет поработать с hadoop, а именно:\n",
    "- Сложить в хранилище hdfs файлы для выполнения задания\n",
    "- Использовать его команды на практике\n",
    "- Написать скрипты для MapReduce задачи и добиться их успешного выполнения\n",
    "\n",
    "**Важно!**\n",
    "Старайтесь выполнить все задания в 1 ноутбуке. От всех запущенных комманд и скриптов нужны логи успешного выполнения этих команд. Так что, пожалуйста, не очищаейте логи ячеек перед отправкой.\n",
    "\n",
    "Также структура и порядок ячеек в ноутбуке должен быть выстроен таким образом, чтобы мы сделали **Ran All** по вашему ноутбуку, и все ячейки были выполнены (пути до вмонтированных папок и нужных файлов в них мы учтем сами)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e5da7",
   "metadata": {},
   "source": [
    "### Заметки по работе с докером\n",
    "\n",
    "- Если у вас ОС Windows, то для запуска сборки вполне поможет [docker desktop](https://www.docker.com/products/docker-desktop/) (должен открываться без впн), в связке с [git bash](https://gitforwindows.org/) вам станут доступны команды `docker compose` прямо из консоли гит баша, где вы сможете запустить контейнер на своем железе\n",
    "- Запускать юпитер ноутбук из неймноды (свободный порт 8888 будет прокинут в конфиге, через который вы сможете обращаться к запущенному ноутбуку), его можно запустить через команду на неймноде `python3 -m notebook --ip 0.0.0.0 --port 8888 --allow_remote_access=true --no-browser --allow-root --NotebookApp.token=my_secret`, тогда вместо рандомного токена при заходе в ноутбук вам просто можно будет ввести `my_secret`\n",
    "- В текущем компоуз файле будет указан шаблон вмонтированных (volume) директорий для винды, вам же стоит поменять их на пути вашей ОС. Нужные файлы для дз можете добавлять в вмонтированную папку для неймноды, чтоб их было видно успешно внутри докера, и дальше вы могли их закинуть в hdfs.\n",
    "- Также в текущей ситуации доступности докер хаба мы проверили зеркало гугла, образ для этой сборки будет тянуться оттуда (указан в dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19efae20",
   "metadata": {},
   "source": [
    "**1. Смотрим на Hadoop Distributed File System**\n",
    "\n",
    "В рамках этой части вам нужно будет обращаться к HDFS с помощью CLI, разместить файлы для следующих заданий в распределеннй файловой системе и выполнить несколько преобразований над ними.\n",
    "\n",
    "Для работы файлы можно скачать по следующим ссылкам:\n",
    "- Текст Шекспира на английском [ссылка](https://drive.google.com/file/d/19-ohhp5APaIgq5tjeyL9kC1vh0ur1Up9/view?usp=sharing)\n",
    "- Логи посещения сайтов юзерами за некоторый промежуток времени [ссылка](https://drive.google.com/file/d/1WXyq5WVSWwJYXPuH4kyAJ5mrR3XgfO_H/view?usp=sharing)\n",
    "- Данные по доходам больших компаний, их филиалов в разных странах [ссылка](https://drive.google.com/file/d/1CuUdGdrayqqO7OBYCRXwcJv64g6EhIzJ/view?usp=sharing)\n",
    "\n",
    "Разместите их в нашем внутреннем файловом хранилище докер-контейнера с помощью HDFS CLI, для дальнейшего удобства под каждый файл стоит создать каталог с простым и понятным именем, разместить сами файлы в разных каталогах.\n",
    "\n",
    "Набор комманд, которые вам могут в этом помочь, доступны [здесь](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "\n",
    "В ячейках ниже должен быть полный набор комманд ваших обращей к консоли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c0f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:40 /input\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:42 /output\r\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:34 /tmp\r\n"
     ]
    }
   ],
   "source": [
    "## вы можете обращаться к консоли из ноутбука таким способом\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a5a267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:40 /input\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:42 /output\n",
      "drwxr-xr-x   - root supergroup          0 2024-06-01 07:34 /tmp\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## или же использовать для этого меджик строчку в ячейке %%bash, как вам будет удобнее\n",
    "\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61786cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup         24 2024-06-01 07:40 /input/input.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -R -h /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ваше решение здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f83ac",
   "metadata": {},
   "source": [
    "**2. Решаем задачи MapReduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990332ef",
   "metadata": {},
   "source": [
    "**2.1 Подсчет слов в тексте**\n",
    "\n",
    "В рамках данного задания вам нужно подсчитать кол-во слов в тексте Шекспира (файл приложен к ДЗ в архиве datasets), то есть необходимо реализовать базовый функционал утилиты word count.\n",
    "\n",
    "**Важно** - подсчитывайте число только тех слов, длина которых больше 4 символов. Проводить процесс удаления знаков препинания и прочих символов **не нужно**\n",
    "\n",
    "Ниже вам представлены ячейки, в которых вы должны описать структуру маппера/редьсюера и ниже вызвать их в bash-скрипте запуска MR-таски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "## сюда вы пишете питоновский скрипт для работы маппера\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bef060",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "## сюда вы пишете питоновский скрипт для работы редьюсера\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b35cd9",
   "metadata": {},
   "source": [
    "В качестве проверки ваших python-скриптов до запуска MR таски можно произвести их запуск через консольные команды \n",
    "\n",
    "Тогда наша задача не будет выполняться через датаноды, а только на локальной машине, но в случае ошибок в скриптах вы увидите их и сможете исправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5975e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## пример запуска скриптов на неймноде для проверки их работы\n",
    "\n",
    "cat file | python3 mapper.py | sort -k1,1 (с нужными ключами сортировки) | python3 reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279f6c1",
   "metadata": {},
   "source": [
    "Как только в данной проверке вы получите успешный и корректный результат, можете запустить Map Reduce в ячейке ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## шаблон для запуска MR таски\n",
    "\n",
    "# обязательная чистка директории, куда будем складывать результат отрабоки mr\n",
    "hdfs dfs -rm -r /word_count_task || true\n",
    "\n",
    "# запус mr таски с указанием пути до нужного jar\n",
    "hadoop jar /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
    "    -D mapreduce.job.name=\"word-count\" \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input ## тут указать путь до нужного файла в hdfs\n",
    "    -output /word_count_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c0ff5",
   "metadata": {},
   "source": [
    "Мониторить процесс работы таски можно на nodemanager по порту 8088 (уже прокинут в конфиге), там будет UI, в котором будет видно вашу запущенную задачу и её статус."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62cdb6",
   "metadata": {},
   "source": [
    "Результат работы скрипта должен выглядеть следующим образом:\n",
    "\n",
    "```bash\n",
    "word count\n",
    "abtr 6852\n",
    "btoad 4237\n",
    "stress 1932\n",
    "zen 1885\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести счетчик определенных слов, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "\n",
    "hdfs dfs -cat /word_count_task/* | grep  -E 'lord\\.|god\\.|pray\\.' | sort -t$'\\t' -k2.2nr  | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bd7bd",
   "metadata": {},
   "source": [
    "**2.2 Поиск максимального дохода у филиалов компаний по разным странам**\n",
    "\n",
    "В данном задании нужно поработать с  информацией о доходах больших компаний и их филиалах в разных странных за некоторый период. Данные представлены в след. формате: `страна;компания;дата;доход`. Вам необходимо найти максимальный доход у каждой компании в отдельности по каждой представленной стране в данных.\n",
    "\n",
    "Результат работы скрипта должен выглядеть следующим образом:\n",
    "\n",
    "```bash\n",
    "country  company                    income\n",
    "Austria  Rodriguez, Sims and Arias  123456\n",
    "USA      Rios-Lewis                 789998\n",
    "Canada   Thompson-Mendez            148345\n",
    "```\n",
    "\n",
    "**Рекомендации**\n",
    "\n",
    "1. В рамках этой задачи вам могу пригодиться дополнительные параметры mr таски, отвечающие за настройку шаффла, и правил сортировки ключей внутри него. Почитать о примерах их использования можно [здесь](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#More_Usage_Examples).\n",
    "\n",
    "2. Не рекомендуем использовать `\\t` в качестве символа разделителя для сложного ключа (потому что по дефолту таб используется для разделения колонок данных, и ключом в таком случае будет только первая колонка до таба). Если вы будете собирать сложный ключ для нужной вам сортировки данных, лучше всего будет использовать другие симловы, к примеру `+, =`\n",
    "\n",
    "3. Как и в задании 2.1, вы можете создать набор ячеек, куда занесете код своих мапперы и редьюсеры, и также соберете код запуска MR таски. Нам нужно, чтобы вы предоставили нам в качестве решения этой задачи все эти составляющие в рамках этого ноутбука."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c5866",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1511d296",
   "metadata": {},
   "source": [
    "**2.3 Решаем задачу поиска самых посещаемых сайтов**\n",
    "\n",
    "В данном задании нужно поработать с логом данных о посещении юзерами различных сайтов.\n",
    "Формат данных: `url;временная метка`. Вам нужно вывести топ 5 сайтов по посещаемости в каждую из дат, которая представлена в наших данных.\n",
    "\n",
    "Результат работы скрипта должен выглядеть следующим образом:\n",
    "\n",
    "```bash\n",
    "date        site                            count\n",
    "2024-05-25  https://gonzales-bautista.com/  987\n",
    "2024-05-25  https://smith.com/              654\n",
    "2024-05-25  https://www.smith.com/          321\n",
    "```\n",
    "\n",
    "**Рекомендации**\n",
    "\n",
    "1. Вам могу пригодиться дополнительные параметры mr таски, отвечающие за настройку шаффла, и правил сортировки ключей внутри него. Почитать о примерах их использования можно [здесь](https://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html#More_Usage_Examples).\n",
    "\n",
    "2. Не рекомендуем использовать `\\t` в качестве символа разделителя для сложного ключа (потому что по дефолту таб используется для разделения колонок данных, и ключом в таком случае будет только первая колонка до таба). Если вы будете собирать сложный ключ для нужной вам сортировки данных, лучше всего будет использовать другие симловы, к примеру `+, =`.\n",
    "\n",
    "3. Возможно, у вас не получится решить данную задачу за одну mr таску, тогда вы просто описываете в решении скрипты ваших мапперов, редьюсеров под каждую из mr тасок, которые вам нужно запустить для получения нужного результата.\n",
    "\n",
    "**Важно** помнить, что любой маппер и редьюсер должен работать за O(1) памяти, и если вы будете создавать какой-то список, куда будете складывать какие-то данные, то он не должен быть размера O(n). Если такой момент в вашем решении будет, пожалуйста, поясните его текстово, что с вашими переменными все ок, и у них нет размера O(n). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ваше решение здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffeefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести результат работы по определенным компаниям, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "## укажите путь до той директории на hdfs, куда вы складывали результат\n",
    "\n",
    "!hdfs dfs -cat /output/* | grep -E '2024-05-28|2024-06-02|2024-05-30' | column -t -s$'\\t'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
